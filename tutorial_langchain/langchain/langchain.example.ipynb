{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b412cd",
   "metadata": {},
   "source": [
    "# Building a Documentation Chatbot with LangChain\n",
    "\n",
    "This script demonstrates how to build an intelligent chatbot that queries documentation using LangChain. \n",
    "The chatbot can:\n",
    "- Parse and preprocess Markdown files.\n",
    "- Embed document content for efficient similarity-based retrieval.\n",
    "- Answer detailed, context-aware queries from users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "532dfea4-93b4-41e2-b6c1-5d7f57a4140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import langchain.chat_models as chat_models \n",
    "import langchain.embeddings as embeddings \n",
    "import langchain.chains as chains \n",
    "import langchain_utils as langch_utils\n",
    "\n",
    "import helpers.hdbg as hdbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014b1dd4-f9ec-4263-8f53-657e4a7f64ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING\u001b[0m: Logger already initialized: skipping\n"
     ]
    }
   ],
   "source": [
    "hdbg.init_logger(verbosity=logging.INFO)\n",
    "\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11cac5d3-05aa-4832-975e-483cdda84336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING\u001b[0m: Logger already initialized: skipping\n"
     ]
    }
   ],
   "source": [
    "hdbg.init_logger(verbosity=logging.INFO)\n",
    "\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a42c7e-203e-406d-a315-e5853fd2f000",
   "metadata": {},
   "source": [
    "## Define Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a40064-5415-49c9-b2d5-354a839b9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"open_ai_api_key\": \"your_api_key_here\",\n",
    "    # Define language model arguments.\n",
    "    \"language_model\": {\n",
    "        # Define your model here.\n",
    "        \"model\": \"gpt-40-mini\",\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    "    # Define input directory path containing documents.\n",
    "    \"source_directory\": \"../../docs\",\n",
    "    \"parse_data_into_chunks\": {\n",
    "        \"chunk_size\" = 500,\n",
    "        \"chunk_overlap\" = 50,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208ea51-4419-4137-89a2-5f2756c14f1d",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "We'll begin by importing the required libraries and configuring the environment. The chatbot will use:\n",
    "- OpenAI's GPT-4o-mini as the core language model.\n",
    "- FAISS for fast document retrieval.\n",
    "- LangChain utilities for document parsing, text splitting, and chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac548ed5-8093-43b0-a498-e17ebb61b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key.\n",
    "os.environ[\"OPENAI_API_KEY\"] = config[\"open_ai_api_key\"]\n",
    "# Initialize the chat model.\n",
    "chat_model = chatmodels.ChatOpenAI(**config[\"language_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542cfc8",
   "metadata": {},
   "source": [
    "## Parse and Preprocess Documentation\n",
    "\n",
    "Markdown files serve as the primary data source for this chatbot. \n",
    "We'll parse the files into LangChain `Document` objects and split them into manageable chunks to ensure efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5f91f41-afe7-49fa-9859-397009613558",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lang_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m split_documents \u001b[38;5;241m=\u001b[39m \u001b[43mlang_utils\u001b[49m\u001b[38;5;241m.\u001b[39mparse_data_into_chunks(\n\u001b[1;32m      2\u001b[0m     dir_path \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_directory\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparse_data_into_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m _LOG\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed and chunked \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(split_documents))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print sample chunked documents\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lang_utils' is not defined"
     ]
    }
   ],
   "source": [
    "split_documents = langch_utils.parse_data_into_chunks(\n",
    "    dir_path = config[\"source_directory\"],\n",
    "    **config[\"parse_data_into_chunks\"],\n",
    ")\n",
    "_LOG.info(\"Processed and chunked %d documents.\", len(split_documents))\n",
    "# Print sample chunked documents.\n",
    "for doc in split_documents[:5]:\n",
    "    _LOG.info(\"Source: %s\", {doc.metadata['source']})\n",
    "    _LOG.info(\"Content: %s\", {doc.page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31074294",
   "metadata": {},
   "source": [
    "## Create a FAISS Vector Store\n",
    "\n",
    "To enable fast document retrieval, we'll embed the document chunks using OpenAI's embeddings and store them in a FAISS vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63a3a326-140a-4543-8f01-155e0fa36192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING\u001b[0m: Logger already initialized: skipping\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI embeddings.\n",
    "embeddings = embeddings.OpenAIEmbeddings()\n",
    "# Create a FAISS vector store.\n",
    "vector_store = langch_utils.create_vector_store(chunked_documents, embeddings)\n",
    "logger.info(\"FAISS vector store created with %d documents.\", len(chunked_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23fd80",
   "metadata": {},
   "source": [
    "## Build a QA Chain\n",
    "\n",
    "The `RetrievalQA` chain combines document retrieval with OpenAI's GPT-3.5 for question answering. \n",
    "It retrieves the most relevant document chunks and uses them as context to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bab54142-1414-4d82-a6e1-1a51db624dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the retriever from the vector store.\n",
    "retriever = langch_utils.build_retriever(vector_store)\n",
    "\n",
    "# Create the RetrievalQA chain.\n",
    "qa_chain = chains.RetrievalQA.from_chain_type(llm=chat_model, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "logger.info(\"RetrievalQA chain initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e262aba",
   "metadata": {},
   "source": [
    "## Step 5: Query the Chatbot\n",
    "\n",
    "Let's interact with the chatbot! We'll ask it questions based on the documentation. \n",
    "The chatbot will retrieve relevant chunks and generate context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427c7461-6d5c-4125-a2f4-2a57e5b62b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a user query.\n",
    "query = \"What are the guidelines for setting up a new project?\"\n",
    "\n",
    "# Query the chatbot.\n",
    "response = qa_chain({\"query\": query})\n",
    "\n",
    "# Display the answer and source documents.\n",
    "print(f\"Answer:\\n{response['result']}\\n\")\n",
    "print(\"Source Documents:\")\n",
    "for doc in response['source_documents']:\n",
    "    print(f\"- Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d2ca4",
   "metadata": {},
   "source": [
    "## Step 6: Dynamic Updates\n",
    "\n",
    "What if the documentation changes? We'll handle this by monitoring the folder for new or modified files.\n",
    "The vector store will be updated dynamically to ensure the chatbot stays up-to-date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2c959f-0def-4a38-b329-34bf7f3206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the folder for changes and update the vector store.\n",
    "known_files = {}\n",
    "changes = langch_utils.watch_folder_for_changes(docs_directory, known_files)\n",
    "\n",
    "if changes[\"new\"] or changes[\"modified\"]:\n",
    "    # Parse and process the changed files\n",
    "    new_documents = langch_utils.parse_markdown_files(changes[\"new\"] + changes[\"modified\"])\n",
    "    langch_utils.update_vector_store(vector_store, new_documents, embeddings)\n",
    "    logger.info(\"Vector store updated with new/modified documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260331ce",
   "metadata": {},
   "source": [
    "## Step 7: Enhancements - Personalization\n",
    "\n",
    "We can extend the chatbot to include personalized responses:\n",
    "- Filter documents by metadata (e.g., tags, categories).\n",
    "- Customize responses based on user preferences.\n",
    "\n",
    "For example, users can ask for specific sections of the documentation or request summaries tailored to their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e2de353-2f56-41e4-bf35-4d6c036bd539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {'<!-- toc -->\\n\\n- [Tutorials \"Learn X in 60 minutes\"](#tutorials-learn-x-in-60-minutes)\\n  * [What are the goals for each tutorial](#what-are-the-goals-for-each-tutorial)\\n\\n<!-- tocstop -->\\n\\n# Tutorials \"Learn X in 60 minutes\"\\n\\nThe goal is to give everything needed for one person to become familiar with a\\nBig data / AI / LLM / data science technology in 60 minutes.\\n\\n- Each tutorial conceptually corresponds to a blog entry.'}\n",
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {'Each tutorial corresponds to a directory in the `//tutorials` repo\\n[https://github.com/causify-ai/tutorials](https://github.com/causify-ai/tutorials)\\nwith'}\n",
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {'- A markdown \\\\`XYZ.API.md\\\\` about the API and the software layer written by us\\n  on top of the native API\\n- A markdown `XYZ.example.md` with a full example of an application using the\\n  API\\n- A Docker container with everything you need in our Causify dev-system format\\n- A Jupyter notebook with an example of APIs\\n- A Jupyter notebook with a full example\\n\\n## What are the goals for each tutorial\\n\\nDocker container'}\n",
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {'Docker container\\n\\n- Provides a Docker container with everything installed and ready to run\\n  tutorials and develop with that technology\\n  - Often installing the package and get it to work takes long to figure out\\n- All the code is on GitHub in a common format to all tutorials\\n\\nJupyter notebooks'}\n",
      "INFO  Source: {'../../docs/all.how_write_tutorials.how_to_guide.md'}\n",
      "INFO  Content: {\"- Each Jupyter notebook should\\n  - Be unit tested so that you are guaranteed that it works\\n    - It's super frustrating when a tutorial doesn't work because the version of\\n      the library is not compatible with the code anymore\\n  - Be self-contained and linear: each example is explained thoroughly without\\n    having to jump from tutorial to tutorial\\n    - Each cell and its output is commented and explained\\n  - Run end-to-end after a restart (we can add a unit test for it)\"}\n"
     ]
    }
   ],
   "source": [
    "# Example query with personalized intent.\n",
    "personalized_query = \"Show me onboarding guidelines for new employees.\"\n",
    "\n",
    "# Query the chatbot.\n",
    "personalized_response = qa_chain({\"query\": personalized_query})\n",
    "\n",
    "# Display the personalized response.\n",
    "print(f\"Answer:\\n{personalized_response['result']}\\n\")\n",
    "print(\"Source Documents:\")\n",
    "for doc in personalized_response['source_documents']:\n",
    "    print(f\"- Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e7b90",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this script, we:\n",
    "1. Parsed and processed Markdown documentation.\n",
    "2. Embedded document chunks into a FAISS vector store for efficient retrieval.\n",
    "3. Built a RetrievalQA chain for context-aware question answering.\n",
    "4. Enabled dynamic updates to handle changing documentation.\n",
    "5. Enhanced the chatbot with personalized query handling.\n",
    "\n",
    "This showcases how LangChain can be used to build intelligent, flexible chatbots tailored for specific tasks."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
