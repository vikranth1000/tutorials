{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56bac36-e384-4ebb-a24a-d056639fe101",
   "metadata": {},
   "source": [
    "# Ollama API Interface â€“ Tutorial Notebook\n",
    "\n",
    "This notebook demonstrates how to use the `call_ollama()` function from `ollama_API.py`.  \n",
    "This function provides a simple interface to interact with a local Ollama server (e.g., Mistral-7B) to generate summaries or forecasts from textual prompts.\n",
    "\n",
    "We will:\n",
    "1. Inspect the function signature\n",
    "2. Try a basic test prompt\n",
    "3. Handle errors if the server isn't running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c20c2f-f9fb-47be-8d2f-5b3de61a3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the API function\n",
    "from ollama_API import call_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068e812-205a-4a6e-906b-b890eec91df8",
   "metadata": {},
   "source": [
    "## Function Overview\n",
    "\n",
    "The `call_ollama()` function wraps a call to a locally running Ollama LLM server.  \n",
    "It handles the following automatically:\n",
    "- Selecting the model from an environment variable (`OLLAMA_MODEL`)\n",
    "- Connecting to the Ollama daemon (via `OLLAMA_URL`)\n",
    "- Returning only the model's response string\n",
    "- Raising errors if anything goes wrong\n",
    "\n",
    "Let's inspect the code using Python's `inspect` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b84602-2b82-44ca-bc6f-329411b992cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def call_ollama(prompt: str, model: str = None) -> str:\n",
      "    client = _get_client()\n",
      "    model = model or OLLAMA_MODEL\n",
      "    return _safe_call(model=model, prompt=prompt)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(call_ollama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a914095-a172-4e0e-99c9-b8126414769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "try:\n",
    "    prompt = \"Summarize Bitcoin's recent price action in one sentence.\"\n",
    "    response = call_ollama(prompt)\n",
    "    print(\"LLM Response:\\n\", response)\n",
    "except RuntimeError as e:\n",
    "    print(\"Ollama Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b0bfa-0edb-44be-a0d2-a7902a17e493",
   "metadata": {},
   "source": [
    "## Using with a Financial Prompt\n",
    "\n",
    "Below, we simulate a more realistic trend-summary request using 60 BTC/USDT closing prices.  \n",
    "This prompt is in the same format that the full app sends.\n",
    "\n",
    "This helps test the model's ability to summarize time-series data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
