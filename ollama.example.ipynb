{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee05a67-581e-490e-94e1-b95f0600e249",
   "metadata": {},
   "source": [
    "# Bitcoin Forecasting Prompt Generation (Ollama Example)\n",
    "\n",
    "This notebook demonstrates how to transform historical BTC/USDT price data into prompt–completion pairs that can be used for **fine-tuning or prompting a large language model (LLM)**.\n",
    "\n",
    "It aligns with the project goal of using Ollama to **forecast short-term price movements** based on past trends.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load historical BTC closing prices\n",
    "2. Construct prompts from the past 60 minutes\n",
    "3. Attach the next closing price as the target (completion)\n",
    "4. Format and write to a `.jsonl` file for training or testing\n",
    "\n",
    "We do **not** actually run the LLM in this notebook — instead, we prepare high-quality inputs **for fine-tuning** or **prompt-response evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2a3e84-bdf3-4eaf-9e19-deb79dc642f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b85a037-3949-42c0-814e-607f574d8607",
   "metadata": {},
   "source": [
    "## Step 1 – Load Historical Data\n",
    "\n",
    "We use a historical BTC/USDT CSV file exported from Binance or another exchange.\n",
    "\n",
    "Assumptions:\n",
    "- The CSV file is located at `historical_analysis/data/historical_btc_data.csv`\n",
    "- It has a `start` timestamp column (in milliseconds)\n",
    "- It has a `close` column for the closing price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf68b2-f335-4a85-ac09-038a1357f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "INPUT_CSV = Path(\"data/historical_btc_data.csv\")\n",
    "WINDOW = 60  # Use last 60 closes as context\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Convert timestamp and clean\n",
    "df[\"Time\"] = pd.to_datetime(df[\"start\"], unit=\"ms\", utc=True)\n",
    "series = df.set_index(\"Time\")[\"close\"].astype(float).dropna()\n",
    "\n",
    "# Preview\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f2685c-e344-4592-bd2c-9d108588ecb7",
   "metadata": {},
   "source": [
    "## Step 2 – Create Prompts from Price Windows\n",
    "\n",
    "We'll create a helper function that:\n",
    "- Takes a list of 60 prices\n",
    "- Formats them as a numbered list\n",
    "- Builds a **prompt** string asking the model to forecast the next price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45582de-2423-47a9-9486-6b79ae8ab352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(prices):\n",
    "    \"\"\"\n",
    "    Given a list of floats, builds a prompt string listing them and a completion\n",
    "    asking for the next price.\n",
    "    \"\"\"\n",
    "    lines = [f\"{i+1:02d}. {p:.2f}\" for i, p in enumerate(prices)]\n",
    "    prompt = (\n",
    "        \"Here are the last 60 BTC/USDT closing prices (most recent last):\\n\"\n",
    "        + \"\\n\".join(lines)\n",
    "        + \"\\n\\nPlease forecast the next closing price (just the number).\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b3e77-5466-4c4f-b39b-cefbbdfa3f50",
   "metadata": {},
   "source": [
    "## Step 3 – Generate Prompt–Completion Pairs\n",
    "\n",
    "For every sequence of 60 prices, we attach the **next closing price** as the label.  \n",
    "This structure is used for both **prompt-based forecasting** and **fine-tuning**.\n",
    "\n",
    "We will generate a few samples below and inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbb8c7d-95d3-4bac-b00d-cd9205213b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for i in range(3):  # Just create 3 examples for preview\n",
    "    hist = series.iloc[i : i + WINDOW].tolist()\n",
    "    target = series.iloc[i + WINDOW]\n",
    "    prompt = make_example(hist)\n",
    "    completion = f\" {target:.2f}\"  # space prefix is required\n",
    "    samples.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": completion\n",
    "    })\n",
    "\n",
    "samples[0]  # Show first example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60e92b-fd68-4c60-b45f-1228034e2b96",
   "metadata": {},
   "source": [
    "## Sample Output\n",
    "\n",
    "This is a single prompt–completion pair that we would send to an LLM or use to fine-tune a forecasting head.\n",
    "\n",
    "- The **prompt** gives the last 60 BTC closing prices in order.\n",
    "- The **completion** is a single number: the next price.\n",
    "\n",
    "This lets the model learn short-term patterns from raw prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d18c47-5db6-4926-a7ae-d0463ea19761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print all three samples\n",
    "for i, sample in enumerate(samples, 1):\n",
    "    print(f\"\\n--- Sample #{i} ---\")\n",
    "    print(\"Prompt:\\n\", sample[\"prompt\"])\n",
    "    print(\"Completion:\\n\", sample[\"completion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914995e4-8965-4534-921c-81e37b9f35c4",
   "metadata": {},
   "source": [
    "## Step 4 – Write to JSONL (Optional)\n",
    "\n",
    "This step saves all prompt–completion pairs into a `.jsonl` file named `finetune_data.jsonl`.\n",
    "\n",
    "This format is compatible with:\n",
    "- Ollama fine-tuning\n",
    "- OpenAI fine-tune APIs\n",
    "- Any system that takes \"prompt\" → \"completion\" training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45b144-99b1-4aae-93c2-580d3e5c910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSONL = Path(\"finetune_data.jsonl\")\n",
    "\n",
    "with open(OUTPUT_JSONL, \"w\") as out:\n",
    "    for i in range(len(series) - WINDOW):\n",
    "        hist = series.iloc[i : i + WINDOW].tolist()\n",
    "        target = series.iloc[i + WINDOW]\n",
    "        prompt = make_example(hist)\n",
    "        completion = f\" {target:.2f}\"\n",
    "        out.write(json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion\n",
    "        }) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {OUTPUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc5c22-848e-41da-8257-f8dd242b861e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- This notebook demonstrates how to prepare time-series data for LLM training or inference.\n",
    "- The format follows Ollama and OpenAI fine-tune requirements.\n",
    "- The `.jsonl` output can be used to:\n",
    "  - Fine-tune a local model (e.g. Mistral, LLaMA)\n",
    "  - Evaluate prompt performance on holdout data\n",
    "  - Generate forecasts based on prompt-only workflows\n",
    "\n",
    "> You can now move to a model script or dashboard and test how the LLM performs on these prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150822fa-1e3b-433d-9549-04b76b77ffa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
